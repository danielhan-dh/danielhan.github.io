<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Homework 2</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 0;
            background-color: #f7f7f7;
        }
        .container {
            width: 80%;
            background-color: #fff;
            padding: 20px;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            text-align: center;
        }
        h1, h2 {
            margin-bottom: 10px;
        }
        .code-block {
            background-color: #f1f1f1;
            border-radius: 8px;
            padding: 20px;
            text-align: left;
            font-family: "Courier New", Courier, monospace;
            white-space: pre-wrap;
            word-wrap: break-word;
            margin-bottom: 20px;
        }
        .code-block code {
            color: #d63384;
        }
        p {
            text-align: justify;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>

    <div class="container">
        <h1>Homework 2</h1>
        <h2>Question 1</h2>
        <p>Presented below is code of gradient boosting on a locally weighted regression algorithm. This code is then applied onto the dataset "concrete" which contains various feature variables detailing the attributes of concrete. These feature variables are then used to train and predict the y value "strength".</p>

        <div class="code-block">
            <code>
                def loess(xtrain, ytrain, kern, tau, scalerType, alpha):<br>
                    distances = cdist(xtrain, xtrain, metric='Euclidean')<br>
                    scaler = scalerType<br>
                    xscaled = scaler.fit_transform(xtrain)<br>
                    weights = kernel_function(distances, kern, tau=0.05)<br>
                    model = Ridge(alpha=alpha, max_iter=5000)<br>
                    models = []<br>
                    predictions = []<br>
                    for i in range(len(xtrain)):<br>
                        loess = model.fit(np.diag(weights[:, i]) @ xscaled, np.diag(weights[:, i]) @ ytrain)<br>
                        predictions[i] = models[i].predict(X[i].reshape(1, -1))<br>
                        models.append(loess)<br>
                    return models, weights, scaler, predictions
            </code>
        </div>

        <p>Here is the code for the loess model created in homework 1. In this method, it scales the data and computes the distances to fit the loess model.</p>

        <h2>Implementation of Gradient Boosting</h2>
        <p>Below is the implementation of the gradient boosting into the loess model. This includes the methods <code>fit</code>, <code>is_fitted</code>, and <code>predict</code>, as well as the method <code>cross_validate</code> to cross-validate the data at the end.</p>

        <div class="code-block">
            <code>
                class LOESS:<br>
                    def __init__(self, kern='tricube', tau=0.05, alpha=0.01, scalerType=StandardScaler()):<br>
                        self.kern = kern<br>
                        self.tau = tau<br>
                        self.alpha = alpha<br>
                        self.scalerType = scalerType<br>
                        self.is_fitted_ = False  # To track if the model has been fitted<br>
                        self.models = []<br>
                        self.scaler = None<br>
                    <br>
                    def fit(self, X, y, boosting_steps=10):<br>
                        self.scaler = self.scalerType<br>
                        X_scaled = self.scaler.fit_transform(X)<br>
                        residuals = y.copy()<br>
                        for step in range(boosting_steps):<br>
                            distances = cdist(X_scaled, X_scaled, metric='euclidean')<br>
                            weights = kernel_function(distances, self.kern, tau=self.tau)<br>
                            model = Ridge(alpha=self.alpha, max_iter=5000)<br>
                            predictions = np.zeros(len(X_scaled))<br>
                            for i in range(len(X_scaled)):<br>
                                loess_model = model.fit(np.diag(weights[:, i]) @ X_scaled, np.diag(weights[:, i]) @ residuals)<br>
                                predictions[i] = loess_model.predict(X_scaled[i].reshape(1, -1))<br>
                            residuals -= predictions<br>
                            self.models.append(model)<br>
                        self.is_fitted_ = True<br>
                    <br>
                    def predict(self, X):<br>
                        X_scaled = self.scaler.transform(X)<br>
                        final_predictions = np.zeros(len(X_scaled))<br>
                        for model in self.models:<br>
                            step_predictions = np.zeros(len(X_scaled))<br>
                            for i in range(len(X_scaled)):<br>
                                step_predictions[i] = model.predict(X_scaled[i].reshape(1, -1))<br>
                            final_predictions += step_predictions<br>
                        return final_predictions / len(self.models)<br>
                    <br>
                    def is_fitted(self):<br>
                        return self.is_fitted_<br>
                    <br>
                    def cross_validate(self, X, y, n_splits=10):<br>
                        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)<br>
                        mse_scores = []<br>
                        for train_index, val_index in kf.split(X):<br>
                            X_train, X_val = X[train_index], X[val_index]<br>
                            y_train, y_val = y[train_index], y[val_index]<br>
                            self.fit(X_train, y_train, boosting_steps=10)<br>
                            y_pred = self.predict(X_val)<br>
                            mse = mean_squared_error(y_val, y_pred)<br>
                            mse_scores.append(mse)<br>
                        return np.mean(mse_scores)
            </code>
        </div>

        <p>Gradient boosting is applied in the <code>fit</code> and <code>predict</code> methods. The method <code>fit</code> applies the loss function, and the residuals of the model represent how far away the predictions are from the true y values. The function performs this for <code>boosting_steps</code> number of iterations, which is optimized later in the code.</p>

        <h2>Scaler Testing</h2>
        <p>Below is the code for testing which scalar has the best MSE out of <code>StandardScaler</code>, <code>MinMaxScaler</code>, and <code>QuantileTransformer</code>.</p>

        <div class="code-block">
            <code>
                scalers = [StandardScaler(), MinMaxScaler(), QuantileTransformer()]<br>
                mse_scalers = []<br>
                for scaler in scalers:<br>
                    loess_model = LOESS(kern=Quartic, tau=0.05, alpha=0.01, scalerType=scaler)<br>
                    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)<br>
                    loess_model.fit(X_train, y_train, boosting_steps=10)<br>
                    predictions = loess_model.predict(X_test)<br>
                    mse = mean_squared_error(y_test, predictions)<br>
                    mse_scalers.append(mse)
            </code>
        </div>

        <p>StandardScaler had an MSE of 1502.299, MinMaxScaler an MSE of 1299.467, and QuantileTransformer an MSE of 1311.889. Therefore, the best scaler to use on the concrete dataset is <code>MinMaxScaler</code>.</p>

        <h2>Kernel Testing</h2>
        <p>The code below tests for the best kernel out of Gaussian, Tricubic, Epanechnikov, and Quartic.</p>

        <div class="code-block">
            <code>
                kernels = [Gaussian, tricubic, Epanechnikov, Quartic]<br>
                mse_kernel = []<br>
                for kernel in kernels:<br>
                    loess_model = LOESS(kern=kernel, tau=0.05, alpha=0.01, scalerType=MinMaxScaler())<br>
                    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)<br>
                    loess_model.fit(X_train, y_train, boosting_steps=10)<br>
                    predictions = loess_model.predict(X_test)<br>
                    mse = mean_squared_error(y_test, predictions)<br>
                    mse_kernel.append(mse)
            </code>
        </div>

       <p>Interestingly, the kernel did not matter as much as the scalar did, as all the MSE values had the same value at 1299.467, the same as the MSE from just testing the <code>MinMaxScaler</code>. Further research is required to find out why this is, as the kernel should generally change the outcome. Normally, the <code>Epanechnikov</code> kernel would yield the lowest MSE out of all the kernels.</p>

        <h2>Hyperparameter Testing</h2>
        <p>The code below shows the testing steps to determine the best hyperparameters. Here, I tested for the optimal number of boosting steps, tau, and alpha. The steps I chose were from 5-10, for tau 0.05-0.1, and for alpha 0.01 to 0.1. I did not want to include a huge range for any of these hyperparameters because the execution time was increasing exponentially. This was the best range I found that would execute in a timely manner while also covering a wide range of hyperparameters.</p>

        <div class="code-block">
            <code>
                best_n_steps = None<br>
                best_tau = None<br>
                best_alpha = None<br>
                best_mse = float('inf')<br>
                <br>
                for n_steps in [5, 10]:<br>
                    for tau in [0.05, 0.1]:<br>
                        for alpha in [0.01, 0.1]:<br>
                            loess_boost_model = LOESS(kern='tricube', tau=tau, alpha=alpha, scalerType=MinMaxScaler())<br>
                            X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)<br>
                            loess_model.fit(X_train, y_train, boosting_steps=n_steps)<br>
                            <br>
                            predictions = loess_model.predict(X_test)<br>
                            <br>
                            mse = mean_squared_error(y_test, predictions)<br>
                            if mse < best_mse:<br>
                                best_mse = mse<br>
                                best_n_steps = n_steps<br>
                                best_tau = tau<br>
                                best_alpha = alpha
            </code>
        </div>

        <p>At the end of these processes, I determined that the best parameters to be used in the gradient boosting algorithm were 5 steps of boosting, a tau of 0.1, and an alpha of 0.1. This yielded the best MSE of 1158.985. Finally, the best model after all of the testing is shown here:</p>

        <div class="code-block">
            <code>
                loess_model = LOESS(kern=Gaussian, tau=0.1, alpha=0.1, scalerType=MinMaxScaler())
            </code>
        </div>

        <h2>Final Cross Validation</h2>
        <p>After testing it in a 10-split cross validation, the final MSE is 1361.708.</p>

        <h2>Comparison with eXtreme Gradient Boosting</h2>
        <p>Compared to the eXtreme Gradient Boosting (XGBoost), the MSE is significantly higher. The eXtreme model ended with an MSE of 23.730, which is magnitudes lower than my self-implemented gradient boosting and LOESS algorithm. Clearly, there is a critical error within my implementation, but I was not able to determine where this is.</p>

               <h1>Homework 2 - Question 2</h1>

        <h2>Iris Dataset Classification</h2>
        <p>Using the <em>iris</em> dataset, which contains 4 feature variables describing the physical characteristics of irises, I implemented locally weighted logistic regression to classify which species of flower they belonged to based on these features. Below is my implementation of locally weighted logistic regression:</p>

        <div class="code-block">
            <code>
                def lwlr(X, y, x_query, tau):<br>
                    m = len(y)<br>
                    weights = np.array([kernel(x_query, X[i], tau) for i in range(m)])<br>
                    W = np.diag(weights)<br>
                    <br>
                    theta = np.linalg.pinv(X.T @ W @ X) @ X.T @ W @ y<br>
                    return expit(x_query @ theta)
            </code>
        </div>

        <p>To accommodate all possible classifications in this multiclass dataset (which contains 3 possible categories), I used a one-vs-all approach. This means that for each category, the algorithm performs binary classification: 1 vs not 1, 2 vs not 2, or 3 vs not 3.</p>

        <div class="code-block">
            <code>
                def predict_multiclass_lwlr(X, y, X_query, tau):<br>
                    n_classes = len(np.unique(y))<br>
                    predictions = []<br>
                    <br>
                    for c in range(n_classes):<br>
                        binary_y = np.where(y == c, 1, 0)<br>
                        class_preds = [lwlr(X, binary_y, x_query, tau) for x_query in X_query]<br>
                        predictions.append(class_preds)<br>
                    <br>
                    predictions = np.array(predictions).T<br>
                    return np.argmax(predictions, axis=1)
            </code>
        </div>

        <p>Testing this implementation on the <em>iris</em> dataset yielded an accuracy of 81.33%. The one-vs-all approach worked effectively for this multiclass problem.</p>

        <h2>One-vs-All Classification</h2>
        <p>As noted, this dataset contains 3 categories, so the one-vs-all method converts the multiclass problem into multiple binary classification problems. Each classifier predicts whether a sample belongs to one specific class or not.</p>

        <h2>Comparison with Calvin Chi’s Algorithm</h2>
        <p>Using Calvin Chi’s algorithm with a similar one-vs-all approach also resulted in an accuracy of 81.33%. Both implementations produced the same result. Below is a visualization of how these algorithms categorized the data. The colored dots represent the true class labels, while the colored sections indicate the algorithm's predictions.</p>
        <p><img src="images/graph2.PNG" alt="Graph of locally weighted logistic regression"></p>

 <h1>Homework 3</h1>
        <h2>Question 1: SCAD Algorithm</h2>
        <p>SCAD stands for smoothly clipped absolute deviation. This algorithm is a regularization and variable selection algorithm, most similar to other algorithms such as LASSO and ElasticNet. The benefit of using SCAD is that it alleviates the bias which affects algorithms like LASSO. Using the SCAD penalty and derivative function found on Andy Jones’ implementation below, I modified previous code used for ElasticNet and adapted it to use the SCAD algorithm.</p>

        <div class="code-block">
            <code>
                # Modified for usage with torch<br>
                def scad_penalty(beta_hat, lambda_val, a_val):<br>
                    is_linear = (torch.abs(beta_hat) <= lambda_val)<br>
                    is_quadratic = (lambda_val < torch.abs(beta_hat)) & (torch.abs(beta_hat) <= a_val * lambda_val)<br>
                    is_constant = (a_val * lambda_val) < torch.abs(beta_hat)<br>
                    <br>
                    linear_part = lambda_val * torch.abs(beta_hat) * is_linear<br>
                    quadratic_part = (2 * a_val * lambda_val * torch.abs(beta_hat) - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic<br>
                    constant_part = (lambda_val**2 * (a_val + 1)) / 2 * is_constant<br>
                    return linear_part + quadratic_part + constant_part<br>
                <br>
                def scad_derivative(beta_hat, lambda_val, a_val):<br>
                    return lambda_val * ((beta_hat <= lambda_val) + (a_val * lambda_val - beta_hat) * ((a_val * lambda_val - beta_hat) > 0) / ((a_val - 1) * lambda_val) * (beta_hat > lambda_val))
            </code>
        </div>

        <p>These methods are then used to calculate the penalty when training the SCAD model. Below is the modified SCAD model that integrates the penalty into the loss function.</p>

        <div class="code-block">
            <code>
                class SCAD(nn.Module):<br>
                    def __init__(self, input_size, alpha=1.0, lambda_val=0.1, a_val=0.5):<br>
                        super(SCAD, self).__init__()<br>
                        self.input_size = input_size<br>
                        self.alpha = alpha<br>
                        self.a_val = a_val<br>
                        self.lambda_val = lambda_val<br>
                        self.linear = nn.Linear(input_size, 1, dtype=torch.float64)<br>
                    <br>
                    def forward(self, x):<br>
                        return self.linear(x)<br>
                    <br>
                    def loss(self, y_pred, y_true):<br>
                        mse_loss = nn.MSELoss()(y_pred, y_true)<br>
                        scad_reg = scad_penalty(self.linear.weight, self.lambda_val, self.a_val).sum()<br>
                        loss = mse_loss + self.alpha * scad_reg<br>
                        return loss<br>
                    <br>
                    def fit(self, X, y, num_epochs=100, learning_rate=0.01):<br>
                        optimizer = optim.SGD(self.parameters(), lr=learning_rate)<br>
                        for epoch in range(num_epochs):<br>
                            self.train()<br>
                            optimizer.zero_grad()<br>
                            y_pred = self(X)<br>
                            loss = self.loss(y_pred, y)<br>
                            loss.backward()<br>
                            optimizer.step()<br>
                            if (epoch + 1) % 10 == 0:<br>
                                print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}")<br>
                    <br>
                    def predict(self, X):<br>
                        self.eval()<br>
                        with torch.no_grad():<br>
                            y_pred = self(X)<br>
                        return y_pred<br>
                    <br>
                    def get_coefficients(self):<br>
                        return self.linear.weight
            </code>
        </div>

        <p>Using the concrete dataset, I trained a SCAD model on a train-test split of the data, with the final test loss at 102.4876. After training the model, I obtained the learned weights to determine which features were useful. The closer a feature's coefficient is to 0, the less important it is.</p>

        <h2>Feature Importance</h2>
        <p>The feature with the lowest importance was the amount of ash in the concrete, with a weight of -0.1623.</p>

        <h2>Question 2: Testing Algorithms on Simulated Data</h2>
        <p>Using the code to generate simulated data from the lecture, I tested the three algorithms: ElasticNet, square root LASSO, and my SCAD implementation. Below is the function to generate correlated features:</p>

        <div class="code-block">
            <code>
                def make_correlated_features(num_samples, p, rho):<br>
                    vcor = []<br>
                    for i in range(p):<br>
                        vcor.append(rho**i)<br>
                    r = toeplitz(vcor)<br>
                    mu = np.repeat(0, p)<br>
                    x = np.random.multivariate_normal(mu, r, size=num_samples)<br>
                    return x
            </code>
        </div>

        <p>The final results were:</p>
        <ul>
            <li>ElasticNet MSE: 1.0150</li>
            <li>Square Root LASSO MSE: 1.0439</li>
            <li>SCAD MSE: 0.6941</li>
        </ul>
        <p>The SCAD algorithm had the lowest MSE and therefore performed the best on this simulated dataset.</p>

        <h2>Question 3: Cross Validation</h2>
        <p>To determine the best weights for the penalty function and cross-validate the results, I created a modified version of a previous cross-validation function to work with tensors and include a model parameter.</p>

        <div class="code-block">
            <code>
                def cross_validate_model(model_class, X_tensor, y_tensor, alpha_vals, num_epochs=100, learning_rate=0.01, k_folds=5):<br>
                    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)<br>
                    best_alpha = None<br>
                    best_mse = float('inf')<br>
                    for alpha in alpha_vals:<br>
                        fold_mses = []<br>
                        for train_idx, val_idx in kf.split(X_tensor):<br>
                            X_train, X_val = X_tensor[train_idx], X_tensor[val_idx]<br>
                            y_train, y_val = y_tensor[train_idx], y_tensor[val_idx]<br>
                            model = model_class(input_size=X_train.shape[1], alpha=alpha)<br>
                            model.fit(X_train, y_train, num_epochs=num_epochs)<br>
                            y_pred = model.predict(X_val)<br>
                            mse = nn.MSELoss()(y_pred, y_val).item()<br>
                            fold_mses.append(mse)<br>
                        mean_mse = np.mean(fold_mses)<br>
                        if mean_mse < best_mse:<br>
                            best_mse = mean_mse<br>
                            best_alpha = alpha<br>
                    return best_alpha, best_mse
            </code>
        </div>
    </div>

</body>
</html>

</body>
</html>
